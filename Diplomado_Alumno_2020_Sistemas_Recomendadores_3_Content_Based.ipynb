{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Diplomado Alumno 2020 Sistemas Recomendadores 3 Content Based.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hernan4444/Hernan4444-diplomado-sistemas-recomendadores-2020-1/blob/master/Diplomado_Alumno_2020_Sistemas_Recomendadores_3_Content_Based.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NC-ceGb8LRLT",
        "colab_type": "text"
      },
      "source": [
        "# Práctica de Sistemas Recomendadores 3: Content based"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mACJbcW8T35p",
        "colab_type": "text"
      },
      "source": [
        "En este práctico, utilizaremos la biblioteca de Python [sklearn](https://scikit-learn.org/stable/), para aprender sobre 2 algoritmos para recomendación basado en contenidos y de unas herramientas para preprocesar los textos. En particula, este practico verá:\n",
        "\n",
        "* TF-IDF\n",
        "* Latent Dirichlet Allocation (LDA)\n",
        "* Uso de Stop Words\n",
        "* Non-negative Matrix Factorization (NMF)\n",
        "\n",
        "\n",
        "**Autor**: Antonio Ossa, Manuel Cartagena y editado por Hernán Valdivieso\n",
        "\n",
        "**Ayudantes**: Manuel Cartagena y Hernán Valdivieso\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dss3ZbIauHwN",
        "colab_type": "text"
      },
      "source": [
        "Nombre: \n",
        "\n",
        "- **Completar**\n",
        "- **Completar**\n",
        "\n",
        "**En caso de hacerlo en parejas y no poner ambos nombres repercutirá en un descuento**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-3HVp9guEsg",
        "colab_type": "text"
      },
      "source": [
        "# Índice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYxhCKYPbBT2",
        "colab_type": "toc"
      },
      "source": [
        ">[Práctica de Sistemas Recomendadores 3: Content based](#scrollTo=NC-ceGb8LRLT)\n",
        "\n",
        ">[Índice](#scrollTo=l-3HVp9guEsg)\n",
        "\n",
        ">[Descargando la información](#scrollTo=IFpEoacrMwQx)\n",
        "\n",
        ">[Revisar archivos descargados](#scrollTo=TJon9T5ZMwRG)\n",
        "\n",
        ">>[Preparar entorno](#scrollTo=7HU7NoDUhnYl)\n",
        "\n",
        ">>[Preprocesamiento de datos](#scrollTo=PUYnjZ1yOY-A)\n",
        "\n",
        ">>[Actividad 1](#scrollTo=ckrxbKlTUVJ8)\n",
        "\n",
        ">[Tf-idf](#scrollTo=f23GriULTHgV)\n",
        "\n",
        ">[LDA](#scrollTo=bxqEz_S0ensc)\n",
        "\n",
        ">>[Actividad 2](#scrollTo=2i60zO2fgyVF)\n",
        "\n",
        ">>[Actividad 3](#scrollTo=th7K6SUbhCkf)\n",
        "\n",
        ">>[Generar recomendaciones](#scrollTo=EuNk3cw3SblR)\n",
        "\n",
        ">>[Actividad 4](#scrollTo=EXwYjORwTr17)\n",
        "\n",
        ">>[Actividad 5](#scrollTo=4AUbaWOpoAvc)\n",
        "\n",
        ">>[Actividad 6](#scrollTo=GfgYZ8SLoFsV)\n",
        "\n",
        ">[Stop words](#scrollTo=gfInn_xVSmZ6)\n",
        "\n",
        ">>[Actividad 7](#scrollTo=yaclTipvxM5E)\n",
        "\n",
        ">>[Actividad 8](#scrollTo=uIfyEAdGxgoK)\n",
        "\n",
        ">>[Actividad bonus de LDA (opcional)](#scrollTo=ncn1B8W5mPWb)\n",
        "\n",
        ">[Otro método: Non-negative Matrix Factorization (NMF)](#scrollTo=X60ZefJhxM5c)\n",
        "\n",
        ">>[Actividad bonus de NMF (Opcional)](#scrollTo=Mz2ybg7KowBA)\n",
        "\n",
        ">>[Actividad 9 (Obligatoria)](#scrollTo=nl155tv3pMnW)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFpEoacrMwQx",
        "colab_type": "text"
      },
      "source": [
        "# Descargando la información\n",
        "\n",
        "Vaya ejecutando cada celda presionando el botón de **Play** o presionando Ctrl+Enter (Linux y Windows) o Command+Enter (Macosx) para descargar las bases de datos.\n",
        "\n",
        "*   Recursos:\n",
        "  * `dictionary.p`\n",
        "  * `dictionary-stemm.p`\n",
        "  * `tfidf_model.p`\n",
        "  * `tfidf_model-stemm.p`\n",
        "*   Dataset:\n",
        "  *  `corpus1.csv`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUlFGZprHneQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Descarga de recursos\n",
        "!curl -L -o 'resources.tar.gz' 'https://github.com/PUC-RecSys-Class/Syllabus/blob/master/Practico%204/files/resources.tar.gz?raw=true'\n",
        "\n",
        "# Descompresión del archivo\n",
        "!tar -xvf resources.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BN0P2xxrH0z8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Descarga del dataset\n",
        "!curl -L -o 'dataset.tar.gz' 'https://github.com/PUC-RecSys-Class/Syllabus/blob/master/Practico%204/files/dataset.tar.gz?raw=true'\n",
        "\n",
        "# Descompresión del archivo\n",
        "!tar -xvf dataset.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJon9T5ZMwRG",
        "colab_type": "text"
      },
      "source": [
        "# Revisar archivos descargados\n",
        "\n",
        "Revisemos el _dataset_ descargado:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zT11_REYOyFO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "corpus_df = pd.read_csv('./corpus1.csv', sep='\\t',\n",
        "                        header=None, encoding='latin',\n",
        "                        names=['id', 'title', 'abstract'])\n",
        "corpus_df.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pSGsVQkyQoVi"
      },
      "source": [
        "Podemos ver que este _dataet: contiene 3 columnas:\n",
        "* **_id_**: identificador de cada texto\n",
        "* **_title_**: título del documento, en este caso, de un _paper_\n",
        "* **_abstract_**: primer párafo del _paper_ que es una representación abreviada, objetiva y precisa del contenido de un documento o recurso, sin interpretación crítica y sin mención expresa del autor del resumen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HU7NoDUhnYl",
        "colab_type": "text"
      },
      "source": [
        "## Preparar entorno\n",
        "Primero es necesario instalar algunas librerías previas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtscg3KuMwRL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install nltk\n",
        "!pip install sklearn\n",
        "!pip install gensim\n",
        "!pip install pandas\n",
        "!pip install numpy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrZhH8Kqtx7_",
        "colab_type": "text"
      },
      "source": [
        "Luego necesitamos importar las librerías a utilizar en este práctico. No se asusten por todas las librerías, iremos explicando lo más importante a medida que se avanza en el práctico."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ii2pB-LO0Xy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "\n",
        "import gensim\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "\n",
        "from collections import Counter\n",
        "from os.path import isfile\n",
        "from textwrap import wrap\n",
        "\n",
        "from gensim import corpora, models, similarities\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import NMF\n",
        "\n",
        "pd.options.display.max_columns = None\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUYnjZ1yOY-A",
        "colab_type": "text"
      },
      "source": [
        "## Preprocesamiento de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eo0B-gBrR9Sf",
        "colab_type": "text"
      },
      "source": [
        "Volvemos a cargar el _dataset_ a utilizar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bONZGsYBR_rn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_df = pd.read_csv('./corpus1.csv', sep='\\t',\n",
        "                        header=None, encoding='latin',\n",
        "                        names=['id', 'title', 'abstract'])\n",
        "corpus_df.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "me-LXrP2Ocjc",
        "colab_type": "text"
      },
      "source": [
        "Luego descargamos las librerías de NLTK necesarias:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ru8N7mZ9exU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUTeZc5jSoIG",
        "colab_type": "text"
      },
      "source": [
        "En este momento estamos bajando un _tokenizador_ específico llamado [Punkt Sentence Tokenizer](https://kite.com/python/docs/nltk.tokenize.punkt). Este será usado a continuación para realizar una cierta tarea con los textos (no vamos a decir cual es porque una actividad es que comenten que hace dado unos ejemplos que mostramos c: ). \n",
        "\n",
        "Lo siguiente es implementar una función que transforme texto no estructurado a una lista de *tokens* procesados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25JLYOGGSOck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_tokens(text):\n",
        "    # Pasar todo a minuscula\n",
        "    lowers = text.lower()\n",
        "    \n",
        "    # Quitar puntuación\n",
        "    no_punctuation = lowers.translate({ord(c): None for c in string.punctuation})\n",
        "    \n",
        "    # Tokenizar \n",
        "    tokens = nltk.word_tokenize(no_punctuation)\n",
        "    \n",
        "    # Retornar resultado\n",
        "    return tokens\n",
        "\n",
        "\n",
        "print(get_tokens(\"I'm a super student for recommender systems!\"))\n",
        "print(get_tokens(\"First sentence. Seconde sentence.\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckrxbKlTUVJ8",
        "colab_type": "text"
      },
      "source": [
        "En el código anterior, para ejecutar `nltk.word_tokenize()` era necesario tener descargado _punkt_. \n",
        "\n",
        "## Actividad 1\n",
        "\n",
        "En función a las frases ingresadas y al resultado impreso, ¿Qué significa _Tokenizar_?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpFOeSpET3BL",
        "colab_type": "text"
      },
      "source": [
        "**Respuesta:** COMPLETAR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gifqhG3_ZO0G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A cada abstract le aplicamos la función de get_tokens\n",
        "corpus_df['tokenized_abstract'] = corpus_df.abstract.map(get_tokens)\n",
        "corpus_df.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkraQ1AwWT86",
        "colab_type": "text"
      },
      "source": [
        "Ahora se tiene que generar un diccionario con todas las palabras del *corpus*. Se recomienda revisar la documentación de gensim y leer cómo usar los diccionarios: [corpora.dictionary](https://radimrehurek.com/gensim/corpora/dictionary.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29j2p32oTTzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dict_file = './resources/dictionary.p'\n",
        "\n",
        "if isfile(dict_file): # Verificar si existe el archivo\n",
        "    dictionary = corpora.dictionary.Dictionary().load(dict_file)\n",
        "    \n",
        "else: # En otro caso, crear el archivo y guardarlo\n",
        "    dictionary = corpora.dictionary.Dictionary(documents=corpus_df.tokenised_abstract.tolist())\n",
        "    dictionary.save(dict_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEDzoAtbZdO1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Texto original\n",
        "print(\"Texto 1\")\n",
        "wrap(str(corpus_df.loc[0][\"tokenized_abstract\"]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDJuYbHuY2rX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Texto pasado por el diccionario\n",
        "print(\"Texto 1\")\n",
        "wrap(str(dictionary.doc2bow(corpus_df.loc[0][\"tokenized_abstract\"])))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1Tbu3OUZ_qo",
        "colab_type": "text"
      },
      "source": [
        "Cuando se hizo `dictionary.doc2bow` se transformó una lista de palabas a un contador de ellas. En donde cada tupla representa `(ID, cantidad de veces)` de modo que se reduce la cantidad de palabras del texto a información numerica. \n",
        "\n",
        "Por ejemplo, la tupla `(30, 5)` indica que la palabra con ID 30 está 5 veces en el texto. Revisando el texto podemos ver que la palabra **\"a\"** es la que está repetida 5 veces. Esto implica que **\"a\"** está asignada al ID 30.\n",
        "\n",
        "Ahora aplicaremos esta función a cada texto del _dataset_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvEkoc9_ZsLy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_df['bow'] = corpus_df.tokenized_abstract.map(dictionary.doc2bow)\n",
        "\n",
        "corpus = corpus_df['bow'].tolist()\n",
        "\n",
        "corpus_df.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f23GriULTHgV",
        "colab_type": "text"
      },
      "source": [
        "# Tf-idf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQqcAAVrfwZw",
        "colab_type": "text"
      },
      "source": [
        "Recordemos que Tf-idf es una medida numérica que expresa cuán relevante es una palabra para un documento en una colección. Ahora, dada la frecuencia de cada palabra en cada texto, se v a utilizar esta ténica para obtener tuplas de la forma `(ID, Tf-idf)` en donde ID será el ID de la palabra igual como estaba antes (por ejemplo **\"a\"** tiene ID 30) y Tf-Idf será el valor dado por este algoritmo a la palabra en cuestión."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7ju5n3xTKtj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf_model_file = 'resources/tfidf_model.p'\n",
        "\n",
        "if isfile(tfidf_model_file):\n",
        "    tfidf_model = models.tfidfmodel.TfidfModel().load(tfidf_model_file)\n",
        "\n",
        "else:\n",
        "    tfidf_model = models.tfidfmodel.TfidfModel(corpus, dictionary=dictionary)\n",
        "    tfidf_model.save(tfidf_model_file)\n",
        "\n",
        "corpus_df['tf_idf'] = tfidf_model[corpus_df.bow.tolist()]\n",
        "corpus_df.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxqEz_S0ensc",
        "colab_type": "text"
      },
      "source": [
        "# LDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmECljYRgt-g",
        "colab_type": "text"
      },
      "source": [
        "A continuación utilizaremos el modelo LDA para identificar 10 tópicos sobre los documentos del dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZI94exTemz4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "topic_number = 10\n",
        "iterations = 200\n",
        "\n",
        "lda_model = models.LdaModel(corpus, num_topics=topic_number,\n",
        "                            id2word=dictionary, passes=5, iterations=iterations)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYCmTUfmHqEw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(lda_model[corpus_df.loc[0].bow])\n",
        "print(lda_model[corpus_df.loc[1].bow])\n",
        "print(lda_model[corpus_df.loc[2].bow])\n",
        "print(lda_model[corpus_df.loc[3].bow])\n",
        "print(lda_model[corpus_df.loc[4].bow])\n",
        "print(lda_model[corpus_df.loc[5].bow])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmC5BQaDIZoW",
        "colab_type": "text"
      },
      "source": [
        "Ahora aplicaremos el algoritmo de LDA para identificar los tópicos a cada documento del _dataset_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIHvccRTIYHD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_df['lda'] = lda_model[corpus_df.bow.tolist()]\n",
        "corpus_df.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2i60zO2fgyVF",
        "colab_type": "text"
      },
      "source": [
        "## Actividad 2\n",
        "\n",
        "**Pregunta:** Explique qué representa la columna `lda`, ¿qué significan cada tupla de números?\n",
        "\n",
        "**Respuesta:** COMPLETAR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGcprvI8hbT_",
        "colab_type": "text"
      },
      "source": [
        "En la siguiente celda se mostrarán 10 tópicos del modelo LDA."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bjxu2Boug4FB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lda_model.print_topics(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "th7K6SUbhCkf",
        "colab_type": "text"
      },
      "source": [
        "## Actividad 3\n",
        "\n",
        "**Pregunta:** ¿Qué representa lo impreso en la celda anterior?\n",
        "\n",
        "**Respuesta:** COMPLETAR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuNk3cw3SblR",
        "colab_type": "text"
      },
      "source": [
        "## Generar recomendaciones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHexXEF-SdH2",
        "colab_type": "text"
      },
      "source": [
        "En esta sección se implementan las funciones necesarias para poder generar recomendaciones dado lo que un usuario ha consumido. De manera artificial, se \"samplearán\" 3 documentos aleatorios que representarán al usuario objetivo (`sample`). Luego tendrás que generar diferentes recomendaciones y evaluar los resultados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwM9JHgpAwwu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Random users\n",
        "samples = corpus_df.sample(3)\n",
        "samples_ids = []\n",
        "\n",
        "for n, (ix, paper) in enumerate(samples.iterrows()):\n",
        "    samples_ids.append(ix)\n",
        "    idx, title, abstract, bow, tf_idf, lda = paper[[\n",
        "        'id', 'title', 'abstract', 'bow', 'tf_idf', 'lda']]\n",
        "    print('%d) %s' % (n+1, title))\n",
        "    print('')\n",
        "    print(\"\\n\".join(wrap(abstract)))\n",
        "    print('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1CoIDJFIsOn",
        "colab_type": "text"
      },
      "source": [
        "Lo anterior son 3 textos tomados al azar. Asumiremso que una persona vió estos 3 textos y ahora vamos a recomendarle **5 nuevos documentos por cada documento visto**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IarDHEPrAwm_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Recommendation functions\n",
        "\n",
        "N = len(dictionary)\n",
        "\n",
        "\n",
        "def to_sparse(matrix):\n",
        "    return csr_matrix([\n",
        "        gensim.matutils.sparse2full(row, length=N)\n",
        "        for row in matrix\n",
        "    ])\n",
        "\n",
        "\n",
        "def make_recommendations(model, metric, neighbors):\n",
        "    M = len(corpus)\n",
        "\n",
        "    X = to_sparse(corpus_df[model].tolist())\n",
        "    document_index = NearestNeighbors(\n",
        "        n_neighbors=(neighbors + 1),\n",
        "        algorithm='brute',\n",
        "        metric=metric).fit(X)\n",
        "    return document_index\n",
        "\n",
        "\n",
        "def print_recommendations(indexes, model):\n",
        "    for n, (ix, paper) in enumerate(samples.iterrows()):\n",
        "        dists, neighbors = indexes.kneighbors([gensim.matutils.sparse2full(paper[model], length=N)])\n",
        "        print(paper['title'])\n",
        "        print('')\n",
        "        print('Documentos cercanos: ')\n",
        "        i = 1\n",
        "        for neighbour in neighbors[0]:\n",
        "            if ix != neighbour:\n",
        "                line = str(i) + \". \" + corpus_df.iloc[neighbour]['title']\n",
        "                print(line)\n",
        "                i += 1\n",
        "        print('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPe8hn3ZTqYO",
        "colab_type": "text"
      },
      "source": [
        "A continuación deberá utilizar las funciones implementadas anteriormente para generar nuevas recomendaciones variando los parámetros del modelo. **Agregue nuevas celdas para cada implementación y/o pregunta.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wocbAgaljoJy",
        "colab_type": "text"
      },
      "source": [
        "Aquí hay 2 ejemplos, puede crear más celdas para hacer las pruebas necesarias:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfHLV4NrA0-U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Recommendation example: TF-IDF con distancia euclidean y solo 5 documentos\n",
        "doc_idx = make_recommendations('tf_idf', 'euclidean', 5)\n",
        "print_recommendations(doc_idx, 'tf_idf')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkSP6I1Chm0L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Recommendation example: LDA  con distancia euclidean y solo 3 documentos\n",
        "doc_idx = make_recommendations('lda', 'euclidean', 3)\n",
        "print_recommendations(doc_idx, 'lda')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXwYjORwTr17",
        "colab_type": "text"
      },
      "source": [
        "## Actividad 4\n",
        "\n",
        "**Pregunta:** Ejecute el modelo utilizando como representación tf-idf y una métrica de distancia euclideana. Modifique el tercer argumento `neighbors` a 10. ¿Qué efecto tiene este cambio en el modelo en las recomendaciones observadas aparte de que hay más documentos recomendados? ¿por qué pasa esto? \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yf0gGZtRJ02e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Código para responder la pregunta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idPWhXh4J83i",
        "colab_type": "text"
      },
      "source": [
        "**Respuesta:** COMPLETAR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AUbaWOpoAvc",
        "colab_type": "text"
      },
      "source": [
        "## Actividad 5\n",
        "\n",
        "**Pregunta:** Eligiendo un valor fijo de neighbors y modelo lda ¿Qué efecto tiene el usar LDA versus TF-IDF en las recomendaciones observadas bajo la misma métrica de distancia?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mYMt-WyGKIaJ",
        "colab": {}
      },
      "source": [
        "# Código para responder la pregunta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0nlr0VLJKIaM"
      },
      "source": [
        "**Respuesta:** COMPLETAR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfgYZ8SLoFsV",
        "colab_type": "text"
      },
      "source": [
        "## Actividad 6\n",
        "\n",
        "**Pregunta:** Pruebe nuevamente con LDA usando 5 tópicos y con 20 tópicos ¿qué efecto tiene el número de tópicos en las recomendaciones observadas?\n",
        "\n",
        "**Importante** Tiene que volver a crear un `LdaModel` pero con 5 tópicos y luego con 20, si hace solo `make_recommendations('lda', 'euclidean', 20)`, **no significa que está usando 20 tópicos** sino que está recomendando 20 documentos. No olvide volver a generar los 3 `samples` que simulan las lecturas del usuario, para que disponga de la columna 'lda' actualizada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YwYX4-nJKP2x",
        "colab": {}
      },
      "source": [
        "# Código para responder la pregunta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "77w2e4M9KP2z"
      },
      "source": [
        "**Respuesta:** COMPLETAR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfInn_xVSmZ6",
        "colab_type": "text"
      },
      "source": [
        "# Stop words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LgcEzIcxM4W",
        "colab_type": "text"
      },
      "source": [
        "A continuación, intentaremos mejorar los resultados obtenidos con LDA eliminando las *stopwords*. ¿Qué son las *stopwords*? Son palabras vacías, sin significado, que no aportan (de manera significativa) al sentido de una frase, como los artículos, pronombres, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNlhgqHaVZuE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTKiDBwpoJ4A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    filtered_words = [\n",
        "        word for word in text if word not in stopwords.words('english')\n",
        "    ]\n",
        "    return filtered_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQFhKTk1xM4p",
        "colab_type": "text"
      },
      "source": [
        "Ahora eliminamos los stopwords de los textos y volvemos a hacer todo el proceso pero con textos diferentes. Este proceso dura aproximadamente **5 minutos**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqQRTEtiUf1n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# Puede que se demore un poco esta celda\n",
        "corpus_df['tokenized_abstract_without_stopwords'] = corpus_df.tokenized_abstract.map(remove_stopwords)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Qf1SZ8DRQ6p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_df.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsAcYiECX0FO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_df['bow_without_stopwords'] = corpus_df.tokenized_abstract_without_stopwords.map(dictionary.doc2bow)\n",
        "corpus_df.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2CWQqabaUFo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = corpus_df['bow_without_stopwords'].tolist()\n",
        "\n",
        "tfidf_model_file_without_stopwords = 'resources/tfidf_model.p'\n",
        "\n",
        "if isfile(tfidf_model_file):\n",
        "    tfidf_model_without_stopwords = models.tfidfmodel.TfidfModel().load(tfidf_model_file)\n",
        "\n",
        "else:\n",
        "    tfidf_model_without_stopwords = models.tfidfmodel.TfidfModel(corpus, dictionary=dictionary)\n",
        "    tfidf_model_without_stopwords.save(tfidf_model_file_without_stopwords)\n",
        "\n",
        "corpus_df['tf_idf_without_stopwords'] = tfidf_model_without_stopwords[corpus_df.bow_without_stopwords.tolist()]\n",
        "corpus_df.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ncp8832Uataa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "topic_number = 10\n",
        "\n",
        "lda_model_without_stopwords = models.LdaModel(corpus, num_topics=topic_number, id2word=dictionary, passes=5, iterations=200)\n",
        "corpus_df['lda_without_stopwords'] = lda_model_without_stopwords[corpus_df.bow_without_stopwords.tolist()]\n",
        "corpus_df.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ah0mFrEWhXgA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lda_model_without_stopwords.print_topics(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaclTipvxM5E",
        "colab_type": "text"
      },
      "source": [
        "## Actividad 7\n",
        "\n",
        "**Pregunta:** ¿Qué puede comentar de estos nuevos tópicos comparándolos con los obtenidos previamente (sección LDA)?\n",
        "\n",
        "**Respuesta:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ODPKrtce-6O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Rellocate user\n",
        "\n",
        "samples = corpus_df.iloc[samples_ids]\n",
        "\n",
        "for n, (ix, paper) in enumerate(samples.iterrows()):\n",
        "    idx, title, abstract, bow, tf_idf, lda, bow_without_stopwords, tf_idf_without_stopwords, lda_without_stopwords = paper[[\n",
        "        'id', 'title', 'abstract', 'bow', 'tf_idf', 'lda', 'bow_without_stopwords', 'tf_idf_without_stopwords', 'lda_without_stopwords']]\n",
        "    print('%d) %s' % (n+1, title))\n",
        "    print('')\n",
        "    print(\"\\n\".join(wrap(abstract)))\n",
        "    print('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scQsJAc7cOhV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Recommendation example: TF-IDF without stopwords\n",
        "\n",
        "doc_idx = make_recommendations('tf_idf_without_stopwords', 'euclidean', 5)\n",
        "print_recommendations(doc_idx, 'tf_idf_without_stopwords')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUXP52_Kckb_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Recommendation example: LDA without stopwords\n",
        "\n",
        "doc_idx = make_recommendations('lda_without_stopwords', 'euclidean', 5)\n",
        "print_recommendations(doc_idx, 'lda_without_stopwords')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIfyEAdGxgoK",
        "colab_type": "text"
      },
      "source": [
        "## Actividad 8\n",
        "\n",
        "**Pregunta:** ¿Cómo cambian las recomendaciones entre ambos métodos ahora que no consideramos las *stopwords*?\n",
        "\n",
        "**Respuesta:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncn1B8W5mPWb",
        "colab_type": "text"
      },
      "source": [
        "## Actividad bonus de LDA (opcional)\n",
        "\n",
        "**Pregunta:** Realice el siguiente gráfico. Pruebe graficando los items con respecto al tópico con mayor probabilidad de pertenencia, para poder hacer el gráfico deberá usar algún método de reducción de dimensionalidad como PCA o T-SNE a los valores de LDA que están en el dataframe.\n",
        "\n",
        "**Importante** Priorice realizar las demás actividades obligatorias antes de esta. En esta lo que tiene que hacer es \n",
        "1. Tomar los valores de LDA  (tomar la columna adecuada)\n",
        "2. Aplicar reducción de dimensión a esos datos para 2 componentes\n",
        "3. Graficar cada punto y luego pintar según el tópico con mayor probabilidad.\n",
        "\n",
        "Ejemplo:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rauMml6Hquox",
        "colab_type": "text"
      },
      "source": [
        "![Expected plot](https://raw.githubusercontent.com/PUC-RecSys-Class/Syllabus/master/Practico%204/files/plot.png)\n",
        "\n",
        "**Importante:** Sigan bajando en el práctico porque hay otras actividades abajo. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LALKKBQcnzlg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Codigo para generar el grafico"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X60ZefJhxM5c",
        "colab_type": "text"
      },
      "source": [
        "# Otro método: Non-negative Matrix Factorization (NMF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rs2eNlQWnAK2",
        "colab_type": "text"
      },
      "source": [
        "A continuación, utilizaremos el modelo NMF para generar recomendaciones:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXlGYDIcjcKi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_components = 10\n",
        "n_top_words = 20\n",
        "\n",
        "\n",
        "def print_top_words(model, feature_names, n_top_words):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        message = \"Tópico #%d: \" % topic_idx\n",
        "        message += \" \".join([feature_names[i]\n",
        "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
        "        print(message)\n",
        "        print()\n",
        "    print()\n",
        "\n",
        "\n",
        "data_samples = corpus_df['abstract'].values\n",
        "\n",
        "# Formato TF-IDF de sklearn\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
        "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
        "\n",
        "# Fit NMF\n",
        "nmf = NMF(n_components=n_components, random_state=1,\n",
        "          alpha=.1, l1_ratio=.5,\n",
        "          ).fit(tfidf)\n",
        "nmf_transform_1 = nmf.transform(tfidf)\n",
        "# Display NMF\n",
        "print(\"Tópicos:\")\n",
        "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
        "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
        "print()\n",
        "print()\n",
        "\n",
        "# Fit NMF with KL-Divergence\n",
        "nmf = NMF(n_components=n_components, random_state=1,\n",
        "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
        "          l1_ratio=.5,\n",
        "          ).fit(tfidf)\n",
        "# Display NMF with KL-Divergence\n",
        "print(\"Tópicos con divergencia KL:\")\n",
        "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
        "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
        "print()\n",
        "print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mwv337wWoX6e",
        "colab_type": "text"
      },
      "source": [
        "Agregamos los valores obtenidos como una columna en el *dataframe*:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96s0zV2PqttB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_df['NMF'] = [[(i, prob) for i, prob in enumerate(l)] for l in nmf_transform_1.tolist()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mz2ybg7KowBA",
        "colab_type": "text"
      },
      "source": [
        "## Actividad bonus de NMF (Opcional)\n",
        "**Pregunta:** En la siguiente casilla de código (`corpus_df.loc[0]['NMF']`) se imprime una lista de tuplas para el documento 0, ¿qué significan las tuplas de dicha lista?\n",
        "\n",
        "**Respuesta:**\n",
        "\n",
        "**Importante**: Siga bajando porque hay una última actividad obligatoria."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5a3NZrLd38Wm",
        "colab": {}
      },
      "source": [
        "corpus_df.loc[0]['NMF']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReZxEDgP4HBQ",
        "colab_type": "text"
      },
      "source": [
        "Ahora observaremos todas las columnas que tiene el _dataframe_. Pueden ver que en la última está `NMF` que es la nueva columna agregada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NR7PS2ws4ciz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXLq5NDiodTp",
        "colab_type": "text"
      },
      "source": [
        "Repetimos el proceso de *sampling* para inspeccionar los resultados:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3kaYZhntt5X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Rellocate user\n",
        "\n",
        "samples = corpus_df.iloc[samples_ids]\n",
        "\n",
        "for n, (ix, paper) in enumerate(samples.iterrows()):\n",
        "    idx, title, abstract, bow, tf_idf, lda, bow_without_stopwords, tf_idf_without_stopwords, lda_without_stopwords, nmf = paper[[\n",
        "        'id', 'title', 'abstract', 'bow', 'tf_idf', 'lda', 'bow_without_stopwords', 'tf_idf_without_stopwords', 'lda_without_stopwords', 'NMF']]\n",
        "    print('%d) %s' % (n+1, title))\n",
        "    print('')\n",
        "    print(\"\\n\".join(wrap(abstract)))\n",
        "    print('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIwj5hu3sVRn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc_idx = make_recommendations('NMF', 'euclidean', 5)\n",
        "print_recommendations(doc_idx, 'NMF')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nl155tv3pMnW",
        "colab_type": "text"
      },
      "source": [
        "## Actividad 9 (Obligatoria)\n",
        "\n",
        "**Pregunta:** Compare y comente sobre las recomendaciones hechas por los métodos anteriores con las obtenidas usando NMF.\n",
        "\n",
        "**Respuesta:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udiRZZeyaino",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}